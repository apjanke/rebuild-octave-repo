# HG changeset patch
# User Rik <rik@octave.org>
# Date 1401724766 25200
#      Mon Jun 02 08:59:26 2014 -0700
# Branch stable
# Node ID 322eb69e30ad76e913d5d3b8bac2cb4312456757
# Parent  dccbc8bff5cb55c599df63c1f37db0860a8e72b4
doc: Fix some Latin wording in documentation.

* external.txi, interp.txi, sparse.txi: Use "a priori" rather than "a-priori".
Don't use "Firstly".

diff --git a/doc/interpreter/external.txi b/doc/interpreter/external.txi
--- a/doc/interpreter/external.txi
+++ b/doc/interpreter/external.txi
@@ -802,17 +802,17 @@ for (int j = 1; j < nc; j++)
           @{
             sm.data(ii) = tmp;
             sm.ridx(ii) = i;
             ii++;
           @}
       @}
     sm.cidx(j+1) = ii;
  @}
-sm.maybe_compress ();  // If don't know a-priori the final # of nz.
+sm.maybe_compress ();  // If don't know a priori the final # of nz.
 @end example
 
 @noindent
 which is probably the most efficient means of creating a sparse matrix.
 
 Finally, it might sometimes arise that the amount of storage initially
 created is insufficient to completely store the sparse matrix.  Therefore,
 the method @code{change_capacity} exists to reallocate the sparse memory.
@@ -842,17 +842,17 @@ for (int j = 1; j < nc; j++)
               @}
             sm.data(ii) = tmp;
             sm.ridx(ii) = i;
             ii++;
           @}
       @}
     sm.cidx(j+1) = ii;
  @}
-sm.maybe_mutate ();  // If don't know a-priori the final # of nz.
+sm.maybe_mutate ();  // If don't know a priori the final # of nz.
 @end example
 
 Note that both increasing and decreasing the number of non-zero elements in
 a sparse matrix is expensive as it involves memory reallocation.  Also as
 parts of the matrix, though not its entirety, exist as old and new copies
 at the same time, additional memory is needed.  Therefore, if possible this
 should be avoided.
 
diff --git a/doc/interpreter/interp.txi b/doc/interpreter/interp.txi
--- a/doc/interpreter/interp.txi
+++ b/doc/interpreter/interp.txi
@@ -83,19 +83,19 @@ interpolation methods for a step functio
 @end ifnotinfo
 
 Fourier interpolation, is a resampling technique where a signal is
 converted to the frequency domain, padded with zeros and then
 reconverted to the time domain.
 
 @DOCSTRING(interpft)
 
-There are two significant limitations on Fourier interpolation.  Firstly,
+There are two significant limitations on Fourier interpolation.  First,
 the function signal is assumed to be periodic, and so non-periodic
-signals will be poorly represented at the edges.  Secondly, both the
+signals will be poorly represented at the edges.  Second, both the
 signal and its interpolation are required to be sampled at equispaced
 points.  An example of the use of @code{interpft} is
 
 @example
 @group
 t = 0 : 0.3 : pi; dt = t(2)-t(1);
 n = length (t); k = 100;
 ti = t(1) + [0 : k-1]*dt*n/k;
diff --git a/doc/interpreter/sparse.txi b/doc/interpreter/sparse.txi
--- a/doc/interpreter/sparse.txi
+++ b/doc/interpreter/sparse.txi
@@ -42,17 +42,17 @@ the speed of the computer and its availa
 the problem size. 
 
 There are many classes of mathematical problems which give rise to
 matrices, where a large number of the elements are zero.  In this case
 it makes sense to have a special matrix type to handle this class of
 problems where only the non-zero elements of the matrix are
 stored.  Not only does this reduce the amount of memory to store the
 matrix, but it also means that operations on this type of matrix can
-take advantage of the a-priori knowledge of the positions of the
+take advantage of the a priori knowledge of the positions of the
 non-zero elements to accelerate their calculations.
 
 A matrix type that stores only the non-zero elements is generally called
 sparse.  It is the purpose of this document to discuss the basics of the
 storage and creation of sparse matrices and the fundamental operations
 on them.
 
 @menu
@@ -68,17 +68,17 @@ on them.
 It is not strictly speaking necessary for the user to understand how
 sparse matrices are stored.  However, such an understanding will help
 to get an understanding of the size of sparse matrices.  Understanding
 the storage technique is also necessary for those users wishing to 
 create their own oct-files. 
 
 There are many different means of storing sparse matrix data.  What all
 of the methods have in common is that they attempt to reduce the complexity
-and storage given a-priori knowledge of the particular class of problems
+and storage given a priori knowledge of the particular class of problems
 that will be solved.  A good summary of the available techniques for storing
 sparse matrix is given by Saad @footnote{Y. Saad "SPARSKIT: A basic toolkit
 for sparse matrix computation", 1994,
 @url{http://www-users.cs.umn.edu/~saad/software/SPARSKIT/paper.ps}}.
 With full matrices, knowledge of the point of an element of the matrix
 within the matrix is implied by its position in the computers memory. 
 However, this is not the case for sparse matrices, and so the positions
 of the non-zero elements of the matrix must equally be stored. 
@@ -568,17 +568,17 @@ Note that the @code{sparse_auto_mutate} 
 
 @node Mathematical Considerations
 @subsubsection Mathematical Considerations
 
 The attempt has been made to make sparse matrices behave in exactly the
 same manner as there full counterparts.  However, there are certain differences
 and especially differences with other products sparse implementations.
 
-Firstly, the @qcode{"./"} and @qcode{".^"} operators must be used with care. 
+First, the @qcode{"./"} and @qcode{".^"} operators must be used with care. 
 Consider what the examples
 
 @example
 @group
   s = speye (4);
   a1 = s .^ 2;
   a2 = s .^ s;
   a3 = s .^ -2;
