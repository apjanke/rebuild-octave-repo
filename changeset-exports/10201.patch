# HG changeset patch
# User Jaroslav Hajek <highegg@gmail.com>
# Date 1264503318 -3600
#      Tue Jan 26 11:55:18 2010 +0100
# Node ID 5c66978f3fdfdae64507d540d7334edb5311331b
# Parent  7c1b1c084af1aa0a95d8ca30c68e4f8ee0a880af
support TypicalX and AutoScaling in fsolve/fminunc, don't autoscale by default

diff --git a/scripts/ChangeLog b/scripts/ChangeLog
--- a/scripts/ChangeLog
+++ b/scripts/ChangeLog
@@ -1,8 +1,16 @@
+2010-01-26  Jaroslav Hajek  <highegg@gmail.com>
+
+	* optimization/fsolve.m: Support TypicalX, autoscale only if
+	AutoScaling is on, off by default. Fix default tolerances.
+	* optimization/fminunc.m: Support TypicalX, autoscale only if
+	AutoScaling is on, off by default Fix default tolerances..
+	* optimization/private/__fdjac__.m: Accept typicalx as a parameter.
+
 2010-01-26  Jaroslav Hajek  <highegg@gmail.com>
 
 	* optimization/pqpnonneg.m: If Cholesky update failed, switch off
 	updating but continue.
 
 2010-01-26  Jaroslav Hajek  <highegg@gmail.com>
 
 	* pkg/pkg.m: More complement -> setdiff changes.
diff --git a/scripts/optimization/fminunc.m b/scripts/optimization/fminunc.m
--- a/scripts/optimization/fminunc.m
+++ b/scripts/optimization/fminunc.m
@@ -27,17 +27,18 @@
 ## In other words, this function attempts to determine a vector @var{x} such 
 ## that @code{@var{fcn} (@var{x})} is a local minimum.
 ## @var{x0} determines a starting guess.  The shape of @var{x0} is preserved
 ## in all calls to @var{fcn}, but otherwise it is treated as a column vector.
 ## @var{options} is a structure specifying additional options.
 ## Currently, @code{fminunc} recognizes these options:
 ## @code{"FunValCheck"}, @code{"OutputFcn"}, @code{"TolX"},
 ## @code{"TolFun"}, @code{"MaxIter"}, @code{"MaxFunEvals"}, 
-## @code{"GradObj"}, @code{"FinDiffType"}.
+## @code{"GradObj"}, @code{"FinDiffType"},
+## @code{"TypicalX"}, @code{"AutoScaling"}.
 ##
 ## If @code{"GradObj"} is @code{"on"}, it specifies that @var{fcn},
 ## called with 2 output arguments, also returns the Jacobian matrix
 ## of right-hand sides at the requested point.  @code{"TolX"} specifies
 ## the termination tolerance in the unknown variables, while 
 ## @code{"TolFun"} is a tolerance for equations.  Default is @code{1e-7}
 ## for both @code{"TolX"} and @code{"TolFun"}.
 ## 
@@ -71,19 +72,20 @@
 
 ## PKG_ADD: __all_opts__ ("fminunc");
 
 function [x, fval, info, output, grad, hess] = fminunc (fcn, x0, options = struct ())
 
   ## Get default options if requested.
   if (nargin == 1 && ischar (fcn) && strcmp (fcn, 'defaults'))
     x = optimset ("MaxIter", 400, "MaxFunEvals", Inf, \
-    "GradObj", "off", "TolX", 1.5e-8, "TolFun", 1.5e-8,
+    "GradObj", "off", "TolX", 1e-7, "TolFun", 1e-7,
     "OutputFcn", [], "FunValCheck", "off",
-    "FinDiffType", "central");
+    "FinDiffType", "central",
+    "TypicalX", [], "AutoScaling", "off");
     return;
   endif
 
   if (nargin < 2 || nargin > 3 || ! ismatrix (x0))
     print_usage ();
   endif    
 
   if (ischar (fcn))
@@ -94,30 +96,41 @@ function [x, fval, info, output, grad, h
   n = numel (x0);
 
   has_grad = strcmpi (optimget (options, "GradObj", "off"), "on");
   cdif = strcmpi (optimget (options, "FinDiffType", "central"), "central");
   maxiter = optimget (options, "MaxIter", 400);
   maxfev = optimget (options, "MaxFunEvals", Inf);
   outfcn = optimget (options, "OutputFcn");
 
+  ## Get scaling matrix using the TypicalX option. If set to "auto", the
+  ## scaling matrix is estimated using the jacobian.
+  typicalx = optimget (options, "TypicalX");
+  if (isempty (typicalx))
+    typicalx = ones (n, 1);
+  endif
+  autoscale = strcmpi (optimget (options, "AutoScaling", "off"), "on");
+  if (! autoscale)
+    dg = 1 ./ typicalx;
+  endif
+
   funvalchk = strcmpi (optimget (options, "FunValCheck", "off"), "on");
 
   if (funvalchk)
     ## Replace fcn with a guarded version.
     fcn = @(x) guarded_eval (fcn, x);
   endif
 
   ## These defaults are rather stringent. I think that normally, user
   ## prefers accuracy to performance.
 
   macheps = eps (class (x0));
 
-  tolx = optimget (options, "TolX", sqrt (macheps));
-  tolf = optimget (options, "TolFun", sqrt (macheps));
+  tolx = optimget (options, "TolX", 1e-7);
+  tolf = optimget (options, "TolFun", 1e-7);
 
   factor = 0.1;
   ## FIXME: TypicalX corresponds to user scaling (???)
   autodg = true;
 
   niter = 1;
   nfev = 0;
 
@@ -152,17 +165,17 @@ function [x, fval, info, output, grad, h
     grad0 = grad;
 
     ## Calculate function value and gradient (possibly via FD).
     if (has_grad)
       [fval, grad] = fcn (reshape (x, xsiz));
       grad = grad(:);
       nfev ++;
     else
-      grad = __fdjac__ (fcn, reshape (x, xsiz), fval, cdif)(:);
+      grad = __fdjac__ (fcn, reshape (x, xsiz), fval, typicalx, cdif)(:);
       nfev += (1 + cdif) * length (x);
     endif
 
     if (niter == 1)
       ## Initialize by identity matrix. 
       hesr = eye (n);
     else
       ## Use the damped BFGS formula.
@@ -174,28 +187,33 @@ function [x, fval, info, output, grad, h
       r = theta * y + (1-theta) * Bs;
       hesr = cholupdate (hesr, r / sqrt (s'*r), "+");
       [hesr, info] = cholupdate (hesr, Bs / sqrt (sBs), "-");
       if (info)
         hesr = eye (n);
       endif
     endif
 
-    ## Second derivatives approximate the hessian.
-    d2f = norm (hesr, 'columns').';
+    if (autoscale)
+      ## Second derivatives approximate the hessian.
+      d2f = norm (hesr, 'columns').';
+      if (niter == 1)
+        dg = d2f;
+      else
+        ## FIXME: maybe fixed lower and upper bounds?
+        dg = max (0.1*dg, d2f);
+      endif
+    endif
+
     if (niter == 1)
-      dg = d2f;
       xn = norm (dg .* x);
       ## FIXME: something better?
       delta = factor * max (xn, 1);
     endif
 
-    ## FIXME: maybe fixed lower and upper bounds?
-    dg = max (0.1*dg, d2f);
-
     ## FIXME -- why tolf*n*xn? If abs (e) ~ abs(x) * eps is a vector
     ## of perturbations of x, then norm (hesr*e) <= eps*xn, i.e. by
     ## tolf ~ eps we demand as much accuracy as we can expect.
     if (norm (grad) <= tolf*n*xn)
       info = 1;
       break;
     endif
 
diff --git a/scripts/optimization/fsolve.m b/scripts/optimization/fsolve.m
--- a/scripts/optimization/fsolve.m
+++ b/scripts/optimization/fsolve.m
@@ -28,24 +28,31 @@
 ## In other words, this function attempts to determine a vector @var{x} such 
 ## that @code{@var{fcn} (@var{x})} gives (approximately) all zeros.
 ## @var{x0} determines a starting guess.  The shape of @var{x0} is preserved
 ## in all calls to @var{fcn}, but otherwise it is treated as a column vector.
 ## @var{options} is a structure specifying additional options.
 ## Currently, @code{fsolve} recognizes these options:
 ## @code{"FunValCheck"}, @code{"OutputFcn"}, @code{"TolX"},
 ## @code{"TolFun"}, @code{"MaxIter"}, @code{"MaxFunEvals"}, 
-## @code{"Jacobian"}, @code{"Updating"} and @code{"ComplexEqn"}.
+## @code{"Jacobian"}, @code{"Updating"}, @code{"ComplexEqn"}
+## @code{"TypicalX"}, @code{"AutoScaling"} and @code{"FinDiffType"}.
 ##
 ## If @code{"Jacobian"} is @code{"on"}, it specifies that @var{fcn},
 ## called with 2 output arguments, also returns the Jacobian matrix
 ## of right-hand sides at the requested point.  @code{"TolX"} specifies
 ## the termination tolerance in the unknown variables, while 
 ## @code{"TolFun"} is a tolerance for equations.  Default is @code{1e-7}
 ## for both @code{"TolX"} and @code{"TolFun"}.
+##
+## If @code{"AutoScaling"} is on, the variables will be automatically scaled
+## according to the column norms of the (estimated) Jacobian. As a result,
+## TolF becomes scaling-independent. By default, this option is off, because
+## it may sometimes deliver unexpected (though mathematically correct) results.
+## 
 ## If @code{"Updating"} is "on", the function will attempt to use Broyden
 ## updates to update the Jacobian, in order to reduce the amount of jacobian
 ## calculations.
 ## If your user function always calculates the Jacobian (regardless of number
 ## of output arguments), this option provides no advantage and should be set to
 ## false.
 ## 
 ## @code{"ComplexEqn"} is @code{"on"}, @code{fsolve} will attempt to solve
@@ -119,19 +126,20 @@
 
 ## PKG_ADD: __all_opts__ ("fsolve");
 
 function [x, fvec, info, output, fjac] = fsolve (fcn, x0, options = struct ())
 
   ## Get default options if requested.
   if (nargin == 1 && ischar (fcn) && strcmp (fcn, 'defaults'))
     x = optimset ("MaxIter", 400, "MaxFunEvals", Inf, \
-    "Jacobian", "off", "TolX", 1.5e-8, "TolFun", 1.5e-8,
+    "Jacobian", "off", "TolX", 1e-7, "TolFun", 1e-7,
     "OutputFcn", [], "Updating", "on", "FunValCheck", "off",
-    "ComplexEqn", "off", "FinDiffType", "central");
+    "ComplexEqn", "off", "FinDiffType", "central",
+    "TypicalX", [], "AutoScaling", "off");
     return;
   endif
 
   if (nargin < 2 || nargin > 3 || ! ismatrix (x0))
     print_usage ();
   endif    
 
   if (ischar (fcn))
@@ -146,30 +154,41 @@ function [x, fvec, info, output, fjac] =
   has_jac = strcmpi (optimget (options, "Jacobian", "off"), "on");
   cdif = strcmpi (optimget (options, "FinDiffType", "central"), "central");
   maxiter = optimget (options, "MaxIter", 400);
   maxfev = optimget (options, "MaxFunEvals", Inf);
   outfcn = optimget (options, "OutputFcn");
   updating = strcmpi (optimget (options, "Updating", "on"), "on");
   complexeqn = strcmpi (optimget (options, "ComplexEqn", "off"), "on");
 
+  ## Get scaling matrix using the TypicalX option. If set to "auto", the
+  ## scaling matrix is estimated using the jacobian.
+  typicalx = optimget (options, "TypicalX");
+  if (isempty (typicalx))
+    typicalx = ones (n, 1);
+  endif
+  autoscale = strcmpi (optimget (options, "AutoScaling", "off"), "on");
+  if (! autoscale)
+    dg = 1 ./ typicalx;
+  endif
+
   funvalchk = strcmpi (optimget (options, "FunValCheck", "off"), "on");
 
   if (funvalchk)
     ## Replace fcn with a guarded version.
     fcn = @(x) guarded_eval (fcn, x, complexeqn);
   endif
 
   ## These defaults are rather stringent. I think that normally, user
   ## prefers accuracy to performance.
 
   macheps = eps (class (x0));
 
-  tolx = optimget (options, "TolX", sqrt (macheps));
-  tolf = optimget (options, "TolFun", sqrt (macheps));
+  tolx = optimget (options, "TolX", 1e-7);
+  tolf = optimget (options, "TolFun", 1e-7);
 
   factor = 1;
 
   niter = 1;
   nfev = 1;
 
   x = x0(:);
   info = 0;
@@ -206,17 +225,17 @@ function [x, fvec, info, output, fjac] =
       [fvec, fjac] = fcn (reshape (x, xsiz));
       ## If the jacobian is sparse, disable Broyden updating.
       if (issparse (fjac))
         updating = false;
       endif
       fvec = fvec(:);
       nfev ++;
     else
-      fjac = __fdjac__ (fcn, reshape (x, xsiz), fvec, cdif);
+      fjac = __fdjac__ (fcn, reshape (x, xsiz), fvec, typicalx, cdif);
       nfev += (1 + cdif) * length (x);
     endif
 
     ## For square and overdetermined systems, we update a QR
     ## factorization of the jacobian to avoid solving a full system in each
     ## step. In this case, we pass a triangular matrix to __dogleg__.
     useqr = updating && m >= n && n > 10;
 
@@ -224,36 +243,41 @@ function [x, fvec, info, output, fjac] =
       ## FIXME: Currently, pivoting is mostly useless because the \ operator
       ## cannot exploit the resulting props of the triangular factor.
       ## Unpivoted QR is significantly faster so it doesn't seem right to pivot
       ## just to get invariance. Original MINPACK didn't pivot either, at least
       ## when qr updating was used.
       [q, r] = qr (fjac, 0);
     endif
 
-    ## Get column norms, use them as scaling factors.
-    jcn = norm (fjac, 'columns').';
+    if (autoscale)
+      ## Get column norms, use them as scaling factors.
+      jcn = norm (fjac, 'columns').';
+      if (niter == 1)
+        dg = jcn;
+        dg(dg == 0) = 1;
+      else
+        ## Rescale adaptively.
+        ## FIXME: the original minpack used the following rescaling strategy:
+        ##   dg = max (dg, jcn);
+        ## but it seems not good if we start with a bad guess yielding jacobian
+        ## columns with large norms that later decrease, because the corresponding
+        ## variable will still be overscaled. So instead, we only give the old
+        ## scaling a small momentum, but do not honor it.
+
+        dg = max (0.1*dg, jcn);
+      endif
+    endif
+
     if (niter == 1)
-      dg = jcn;
-      dg(dg == 0) = 1;
       xn = norm (dg .* x);
       ## FIXME: something better?
       delta = factor * max (xn, 1);
     endif
 
-    ## Rescale adaptively.
-    ## FIXME: the original minpack used the following rescaling strategy:
-    ##   dg = max (dg, jcn);
-    ## but it seems not good if we start with a bad guess yielding jacobian
-    ## columns with large norms that later decrease, because the corresponding
-    ## variable will still be overscaled. So instead, we only give the old
-    ## scaling a small momentum, but do not honor it.
-
-    dg = max (0.1*dg, jcn);
-
     ## It also seems that in the case of fast (and inhomogeneously) changing
     ## jacobian, the Broyden updates are of little use, so maybe we could
     ## skip them if a big disproportional change is expected. The question is,
     ## of course, how to define the above terms :)
 
     lastratio = 0;
     nfail = 0;
     nsuc = 0;
diff --git a/scripts/optimization/private/__fdjac__.m b/scripts/optimization/private/__fdjac__.m
--- a/scripts/optimization/private/__fdjac__.m
+++ b/scripts/optimization/private/__fdjac__.m
@@ -16,30 +16,30 @@
 ## along with Octave; see the file COPYING.  If not, see
 ## <http://www.gnu.org/licenses/>.
 
 ## -*- texinfo -*-
 ## @deftypefn{Function File} {} __fdjac__ (@var{fcn}, @var{x}, @var{fvec}, @var{err})
 ## Undocumented internal function.
 ## @end deftypefn
 
-function fjac = __fdjac__ (fcn, x, fvec, cdif, err = 0)
+function fjac = __fdjac__ (fcn, x, fvec, typicalx, cdif, err = 0)
   if (cdif)
     err = (max (eps, err)) ^ (1/3);
-    h = max (abs (x), 1)*err; # FIXME?
+    h = typicalx*err;
     fjac = zeros (length (fvec), numel (x));
     for i = 1:numel (x)
       x1 = x2 = x;
       x1(i) += h(i);
       x2(i) -= h(i);
       fjac(:,i) = (fcn (x1)(:) - fcn (x2)(:)) / (x1(i) - x2(i));
     endfor
   else
     err = sqrt (max (eps, err));
-    h = max (abs (x), 1)*err; # FIXME?
+    h = typicalx*err;
     fjac = zeros (length (fvec), numel (x));
     for i = 1:numel (x)
       x1 = x;
       x1(i) += h(i);
       fjac(:,i) = (fcn (x1)(:) - fvec) / (x1(i) - x(i));
     endfor
   endif
 endfunction
