# HG changeset patch
# User Rik <rik@octave.org>
# Date 1447289898 28800
#      Wed Nov 11 16:58:18 2015 -0800
# Node ID 6faaab833605973694a8ce545df8e84544a11456
# Parent  a7dbc4fc373242cc755078105fbd47c3e93fbc6c
fminunc.m: Clean up function to meet Octave coding standards.
* fminunc.m: Rename xsiz variable to xsz.  Use double quotes for strings.
Use in-place accumulation operator +=1 instead of ++ for performance.
Use '__' prefix and postfix for internal functions.  Use two spaces between
end of sentence period and start of next sentence.  Only calculate third
output argument if requested.

diff --git a/scripts/optimization/fminunc.m b/scripts/optimization/fminunc.m
--- a/scripts/optimization/fminunc.m
+++ b/scripts/optimization/fminunc.m
@@ -36,17 +36,17 @@
 ## @var{options} is a structure specifying additional options.  Currently,
 ## @code{fminunc} recognizes these options:
 ## @qcode{"FunValCheck"}, @qcode{"OutputFcn"}, @qcode{"TolX"},
 ## @qcode{"TolFun"}, @qcode{"MaxIter"}, @qcode{"MaxFunEvals"},
 ## @qcode{"GradObj"}, @qcode{"FinDiffType"}, @qcode{"TypicalX"},
 ## @qcode{"AutoScaling"}.
 ##
 ## If @qcode{"GradObj"} is @qcode{"on"}, it specifies that @var{fcn}, when
-## called with 2 output arguments, also returns the Jacobian matrix of partial
+## called with two output arguments, also returns the Jacobian matrix of partial
 ## first derivatives at the requested point.  @code{TolX} specifies the
 ## termination tolerance for the unknown variables @var{x}, while @code{TolFun}
 ## is a tolerance for the objective function value @var{fval}.  The default is
 ## @code{1e-7} for both options.
 ##
 ## For a description of the other options, see @code{optimset}.
 ##
 ## On return, @var{x} is the location of the minimum and @var{fval} contains
@@ -75,77 +75,77 @@
 ## @item -3
 ## The trust region radius became excessively small.
 ## @end table
 ##
 ## Optionally, @code{fminunc} can return a structure with convergence statistics
 ## (@var{output}), the output gradient (@var{grad}) at the solution @var{x},
 ## and approximate Hessian (@var{hess}) at the solution @var{x}.
 ##
-## Application Notes: If have only a single nonlinear equation of one variable
-## then using @code{fminbnd} is usually a better choice.
+## Application Notes: If the objective function is a single nonlinear equation
+## of one variable then using @code{fminbnd} is usually a better choice.
 ##
 ## The algorithm used by @code{fminsearch} is a gradient search which depends
 ## on the objective function being differentiable.  If the function has
 ## discontinuities it may be better to use a derivative-free algorithm such as
 ## @code{fminsearch}.
 ## @seealso{fminbnd, fminsearch, optimset}
 ## @end deftypefn
 
 ## PKG_ADD: ## Discard result to avoid polluting workspace with ans at startup.
 ## PKG_ADD: [~] = __all_opts__ ("fminunc");
 
 function [x, fval, info, output, grad, hess] = fminunc (fcn, x0, options = struct ())
 
   ## Get default options if requested.
-  if (nargin == 1 && ischar (fcn) && strcmp (fcn, 'defaults'))
+  if (nargin == 1 && strcmp (fcn, "defaults"))
     x = optimset ("MaxIter", 400, "MaxFunEvals", Inf,
                   "GradObj", "off", "TolX", 1e-7, "TolFun", 1e-7,
                   "OutputFcn", [], "FunValCheck", "off",
                   "FinDiffType", "central",
                   "TypicalX", [], "AutoScaling", "off");
     return;
   endif
 
   if (nargin < 2 || nargin > 3 || ! ismatrix (x0))
     print_usage ();
   endif
 
   if (ischar (fcn))
     fcn = str2func (fcn, "global");
   endif
 
-  xsiz = size (x0);
+  xsz = size (x0);
   n = numel (x0);
 
   has_grad = strcmpi (optimget (options, "GradObj", "off"), "on");
   cdif = strcmpi (optimget (options, "FinDiffType", "central"), "central");
   maxiter = optimget (options, "MaxIter", 400);
   maxfev = optimget (options, "MaxFunEvals", Inf);
   outfcn = optimget (options, "OutputFcn");
 
-  ## Get scaling matrix using the TypicalX option. If set to "auto", the
-  ## scaling matrix is estimated using the jacobian.
+  ## Get scaling matrix using the TypicalX option.  If set to "auto", the
+  ## scaling matrix is estimated using the Jacobian.
   typicalx = optimget (options, "TypicalX");
   if (isempty (typicalx))
     typicalx = ones (n, 1);
   endif
   autoscale = strcmpi (optimget (options, "AutoScaling", "off"), "on");
   if (! autoscale)
     dg = 1 ./ typicalx;
   endif
 
   funvalchk = strcmpi (optimget (options, "FunValCheck", "off"), "on");
 
   if (funvalchk)
     ## Replace fcn with a guarded version.
     fcn = @(x) guarded_eval (fcn, x);
   endif
 
-  ## These defaults are rather stringent. I think that normally, user
+  ## These defaults are rather stringent.  I think that normally, user
   ## prefers accuracy to performance.
 
   macheps = eps (class (x0));
 
   tolx = optimget (options, "TolX", 1e-7);
   tolf = optimget (options, "TolFun", 1e-7);
 
   factor = 0.1;
@@ -154,25 +154,25 @@ function [x, fval, info, output, grad, h
 
   niter = 1;
   nfev = 0;
 
   x = x0(:);
   info = 0;
 
   ## Initial evaluation.
-  fval = fcn (reshape (x, xsiz));
+  fval = fcn (reshape (x, xsz));
   n = length (x);
 
   if (! isempty (outfcn))
     optimvalues.iter = niter;
     optimvalues.funccount = nfev;
     optimvalues.fval = fval;
     optimvalues.searchdirection = zeros (n, 1);
-    state = 'init';
+    state = "init";
     stop = outfcn (x, optimvalues, state);
     if (stop)
       info = -1;
       break;
     endif
   endif
 
   nsuciter = 0;
@@ -182,45 +182,45 @@ function [x, fval, info, output, grad, h
 
   ## Outer loop.
   while (niter < maxiter && nfev < maxfev && ! info)
 
     grad0 = grad;
 
     ## Calculate function value and gradient (possibly via FD).
     if (has_grad)
-      [fval, grad] = fcn (reshape (x, xsiz));
+      [fval, grad] = fcn (reshape (x, xsz));
       grad = grad(:);
-      nfev ++;
+      nfev += 1;
     else
-      grad = __fdjac__ (fcn, reshape (x, xsiz), fval, typicalx, cdif)(:);
+      grad = __fdjac__ (fcn, reshape (x, xsz), fval, typicalx, cdif)(:);
       nfev += (1 + cdif) * length (x);
     endif
 
     if (niter == 1)
       ## Initialize by identity matrix.
       hesr = eye (n);
     else
       ## Use the damped BFGS formula.
       y = grad - grad0;
       sBs = sumsq (w);
-      Bs = hesr'*w;
-      sy = y'*s;
+      Bs = hesr' * w;
+      sy = y' * s;
       theta = 0.8 / max (1 - sy / sBs, 0.8);
       r = theta * y + (1-theta) * Bs;
-      hesr = cholupdate (hesr, r / sqrt (s'*r), "+");
+      hesr = cholupdate (hesr, r / sqrt (s' * r), "+");
       [hesr, info] = cholupdate (hesr, Bs / sqrt (sBs), "-");
       if (info)
         hesr = eye (n);
       endif
     endif
 
     if (autoscale)
-      ## Second derivatives approximate the hessian.
-      d2f = norm (hesr, 'columns').';
+      ## Second derivatives approximate the Hessian.
+      d2f = norm (hesr, "columns").';
       if (niter == 1)
         dg = d2f;
       else
         ## FIXME: maybe fixed lower and upper bounds?
         dg = max (0.1*dg, d2f);
       endif
     endif
 
@@ -246,42 +246,42 @@ function [x, fval, info, output, grad, h
 
       s = - __doglegm__ (hesr, grad, dg, delta);
 
       sn = norm (dg .* s);
       if (niter == 1)
         delta = min (delta, sn);
       endif
 
-      fval1 = fcn (reshape (x + s, xsiz)) (:);
-      nfev ++;
+      fval1 = fcn (reshape (x + s, xsz))(:);
+      nfev += 1;
 
       if (fval1 < fval)
         ## Scaled actual reduction.
         actred = (fval - fval1) / (abs (fval1) + abs (fval));
       else
         actred = -1;
       endif
 
-      w = hesr*s;
+      w = hesr * s;
       ## Scaled predicted reduction, and ratio.
       t = 1/2 * sumsq (w) + grad'*s;
       if (t < 0)
         prered = -t/(abs (fval) + abs (fval + t));
         ratio = actred / prered;
       else
         prered = 0;
         ratio = 0;
       endif
 
       ## Update delta.
       if (ratio < min (max (0.1, 0.8*lastratio), 0.9))
         delta *= decfac;
         decfac ^= 1.4142;
-        if (delta <= 1e1*macheps*xn)
+        if (delta <= 10*macheps*xn)
           ## Trust region became uselessly small.
           info = -3;
           break;
         endif
       else
         lastratio = ratio;
         decfac = 0.5;
         if (abs (1-ratio) <= 0.1)
@@ -295,87 +295,89 @@ function [x, fval, info, output, grad, h
         ## Successful iteration.
         x += s;
         xn = norm (dg .* x);
         fval = fval1;
         nsuciter++;
         suc = true;
       endif
 
-      niter ++;
+      niter += 1;
 
       ## FIXME: should outputfcn be called only after a successful iteration?
       if (! isempty (outfcn))
         optimvalues.iter = niter;
         optimvalues.funccount = nfev;
         optimvalues.fval = fval;
         optimvalues.searchdirection = s;
-        state = 'iter';
+        state = "iter";
         stop = outfcn (x, optimvalues, state);
         if (stop)
           info = -1;
           break;
         endif
       endif
 
-      ## Tests for termination conditions. A mysterious place, anything
+      ## Tests for termination conditions.  A mysterious place, anything
       ## can happen if you change something here...
 
       ## The rule of thumb (which I'm not sure M*b is quite following)
       ## is that for a tolerance that depends on scaling, only 0 makes
-      ## sense as a default value. But 0 usually means uselessly long
+      ## sense as a default value.  But 0 usually means uselessly long
       ## iterations, so we need scaling-independent tolerances wherever
       ## possible.
 
       ## The following tests done only after successful step.
       if (ratio >= 1e-4)
-        ## This one is classic. Note that we use scaled variables again,
+        ## This one is classic.  Note that we use scaled variables again,
         ## but compare to scaled step, so nothing bad.
         if (sn <= tolx*xn)
           info = 2;
           ## Again a classic one.
         elseif (actred < tolf)
           info = 3;
         endif
       endif
 
     endwhile
   endwhile
 
-  ## When info != 1, recalculate the gradient and hessian using the final x.
+  ## When info != 1, recalculate the gradient and Hessian using the final x.
   if (nargout > 4 && (info == -1 || info == 2 || info == 3))
     grad0 = grad;  
     if (has_grad)
-      [fval, grad] = fcn (reshape (x, xsiz));
+      [fval, grad] = fcn (reshape (x, xsz));
       grad = grad(:);
     else
-      grad = __fdjac__ (fcn, reshape (x, xsiz), fval, typicalx, cdif)(:);
+      grad = __fdjac__ (fcn, reshape (x, xsz), fval, typicalx, cdif)(:);
     endif
     
     if (nargout > 5)
       ## Use the damped BFGS formula.
       y = grad - grad0;
       sBs = sumsq (w);
       Bs = hesr' * w;
       sy = y' * s;
       theta = 0.8 / max (1 - sy / sBs, 0.8);
       r = theta * y + (1-theta) * Bs;
       hesr = cholupdate (hesr, r / sqrt (s' * r), "+");
       hesr = cholupdate (hesr, Bs / sqrt (sBs), "-");
     endif
     ## Return the gradient in the same shape as x
-    grad = reshape (grad, xsiz);
+    grad = reshape (grad, xsz);
   endif
 
   ## Restore original shapes.
-  x = reshape (x, xsiz);
+  x = reshape (x, xsz);
 
-  output.iterations = niter;
-  output.successful = nsuciter;
-  output.funcCount = nfev;
+  if (nargout > 3)
+    output.iterations = niter;
+    output.successful = nsuciter;
+    output.funcCount = nfev;
+  endif
 
   if (nargout > 5)
     hess = hesr'*hesr;
   endif
 
 endfunction
 
 ## A helper function that evaluates a function and checks for bad results.
@@ -392,29 +394,29 @@ function [fx, gx] = guarded_eval (fun, x
   elseif (any (isnan (fx(:))))
     error ("fminunc:isnan", "fminunc: NaN value encountered");
   elseif (any (isinf (fx(:))))
     error ("fminunc:isinf", "fminunc: Inf value encountered");
   endif
 endfunction
 
 
-%!function f = __rosenb (x)
+%!function f = __rosenb__ (x)
 %!  n = length (x);
 %!  f = sumsq (1 - x(1:n-1)) + 100 * sumsq (x(2:n) - x(1:n-1).^2);
 %!endfunction
 %!
 %!test
-%! [x, fval, info, out] = fminunc (@__rosenb, [5, -5]);
+%! [x, fval, info, out] = fminunc (@__rosenb__, [5, -5]);
 %! tol = 2e-5;
 %! assert (info > 0);
 %! assert (x, ones (1, 2), tol);
 %! assert (fval, 0, tol);
 %!test
-%! [x, fval, info, out] = fminunc (@__rosenb, zeros (1, 4));
+%! [x, fval, info, out] = fminunc (@__rosenb__, zeros (1, 4));
 %! tol = 2e-5;
 %! assert (info > 0);
 %! assert (x, ones (1, 4), tol);
 %! assert (fval, 0, tol);
 
 ## Test FunValCheck works correctly
 %!assert (fminunc (@(x) x^2, 1, optimset ("FunValCheck", "on")), 0, eps)
 %!error <non-real value> fminunc (@(x) x + i, 1, optimset ("FunValCheck", "on"))
