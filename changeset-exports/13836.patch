# HG changeset patch
# User Rik <octave@nomad.inbox5.com>
# Date 1320643369 28800
#      Sun Nov 06 21:22:49 2011 -0800
# Node ID 519390f1b67f6abcafa8737b904ef863dc026139
# Parent  fc9f34e174864d8ad93733f65ccb4a3cf0dd1402
# Parent  5289d7c2460db8c55912320ce8a0f56a9e44c9bc
maint: periodic merge of stable to default.

diff --git a/scripts/optimization/optimset.m b/scripts/optimization/optimset.m
--- a/scripts/optimization/optimset.m
+++ b/scripts/optimization/optimset.m
@@ -18,16 +18,67 @@
 ## <http://www.gnu.org/licenses/>.
 
 ## -*- texinfo -*-
 ## @deftypefn  {Function File} {} optimset ()
 ## @deftypefnx {Function File} {} optimset (@var{par}, @var{val}, @dots{})
 ## @deftypefnx {Function File} {} optimset (@var{old}, @var{par}, @var{val}, @dots{})
 ## @deftypefnx {Function File} {} optimset (@var{old}, @var{new})
 ## Create options struct for optimization functions.
+##
+## Valid parameters are:
+## @itemize @bullet
+## @item AutoScaling
+##
+## @item ComplexEqn
+##
+## @item FinDiffType
+##
+## @item FunValCheck
+## When enabled, display an error if the objective function returns a complex
+## value or NaN@.  Must be set to "on" or "off" [default].
+##
+## @item GradObj
+## When set to "on", the function to be minimized must return a second argument
+## which is the gradient, or first derivative, of the function at the point
+## @var{x}.  If set to "off" [default], the gradient is computed via finite
+## differences.
+##
+## @item Jacobian
+## When set to "on", the function to be minimized must return a second argument
+## which is the Jacobian, or first derivative, of the function at the point
+## @var{x}.  If set to "off" [default], the Jacobian is computed via finite
+## differences.
+##
+## @item MaxFunEvals
+## Maximum number of function evaluations before optimization stops.
+## Must be a positive integer.
+##
+## @item MaxIter
+## Maximum number of algorithm iterations before optimization stops.
+## Must be a positive integer.
+##
+## @item OutputFcn
+## A user-defined function executed once per algorithm iteration.
+##
+## @item TolFun
+## Termination criterion for the function output.  If the difference in the
+## calculated objective function between one algorithm iteration and the next
+## is less than @code{TolFun} the optimization stops.  Must be a positive
+## scalar.
+##
+## @item TolX
+## Termination criterion for the function input.  If the difference in @var{x},
+## the current search point, between one algorithm iteration and the next is
+## less than @code{TolX} the optimization stops.  Must be a positive scalar.
+##
+## @item TypicalX
+##
+## @item Updating
+## @end itemize
 ## @end deftypefn
 
 function retval = optimset (varargin)
 
   nargs = nargin ();
 
   ## Add more as needed.
   opts = __all_opts__ ();
diff --git a/scripts/statistics/base/var.m b/scripts/statistics/base/var.m
--- a/scripts/statistics/base/var.m
+++ b/scripts/statistics/base/var.m
@@ -18,25 +18,25 @@
 
 ## -*- texinfo -*-
 ## @deftypefn  {Function File} {} var (@var{x})
 ## @deftypefnx {Function File} {} var (@var{x}, @var{opt})
 ## @deftypefnx {Function File} {} var (@var{x}, @var{opt}, @var{dim})
 ## Compute the variance of the elements of the vector @var{x}.
 ## @tex
 ## $$
-## {\rm std} (x) = \sigma^2 = {\sum_{i=1}^N (x_i - \bar{x})^2 \over N - 1}
+## {\rm var} (x) = \sigma^2 = {\sum_{i=1}^N (x_i - \bar{x})^2 \over N - 1}
 ## $$
 ## where $\bar{x}$ is the mean value of $x$.
 ## @end tex
 ## @ifnottex
 ##
 ## @example
 ## @group
-## std (x) = 1/(N-1) SUM_i (x(i) - mean(x))^2
+## var (x) = 1/(N-1) SUM_i (x(i) - mean(x))^2
 ## @end group
 ## @end example
 ##
 ## @end ifnottex
 ## If @var{x} is a matrix, compute the variance for each column
 ## and return them in a row vector.
 ##
 ## The argument @var{opt} determines the type of normalization to use.
