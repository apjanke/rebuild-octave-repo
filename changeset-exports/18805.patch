# HG changeset patch
# User John W. Eaton <jwe@octave.org>
# Date 1402062191 14400
#      Fri Jun 06 09:43:11 2014 -0400
# Branch gui-release
# Node ID d67f33a4009dc819e6a6d2c86848b31dee52cdaa
# Parent  332450e56698d8969d1cb7e193721e06c0288534
# Parent  2dcc4398950d45e080c19b043782db75aa9a650e
maint: Periodic merge of stable to gui-release.

diff --git a/doc/interpreter/external.txi b/doc/interpreter/external.txi
--- a/doc/interpreter/external.txi
+++ b/doc/interpreter/external.txi
@@ -677,20 +677,20 @@ technique
 @group
 int nz, nr, nc;
 nz = 4, nr = 3, nc = 4;
 
 ColumnVector ridx (nz);
 ColumnVector cidx (nz);
 ColumnVector data (nz);
 
-ridx(0) = 0; cidx(0) = 0; data(0) = 1; 
-ridx(1) = 0; cidx(1) = 1; data(1) = 2; 
-ridx(2) = 1; cidx(2) = 3; data(2) = 3; 
-ridx(3) = 2; cidx(3) = 3; data(3) = 4;
+ridx(0) = 1; cidx(0) = 1; data(0) = 1;
+ridx(1) = 2; cidx(1) = 2; data(1) = 2;
+ridx(2) = 2; cidx(2) = 4; data(2) = 3;
+ridx(3) = 3; cidx(3) = 4; data(3) = 4;
 SparseMatrix sm (data, ridx, cidx, nr, nc);
 @end group
 @end example
 
 @noindent
 which creates the matrix given in section
 @ref{Storage of Sparse Matrices}.  Note that the compressed matrix
 format is not used at the time of the creation of the matrix itself,
@@ -802,17 +802,17 @@ for (int j = 1; j < nc; j++)
           @{
             sm.data(ii) = tmp;
             sm.ridx(ii) = i;
             ii++;
           @}
       @}
     sm.cidx(j+1) = ii;
  @}
-sm.maybe_compress ();  // If don't know a-priori the final # of nz.
+sm.maybe_compress ();  // If don't know a priori the final # of nz.
 @end example
 
 @noindent
 which is probably the most efficient means of creating a sparse matrix.
 
 Finally, it might sometimes arise that the amount of storage initially
 created is insufficient to completely store the sparse matrix.  Therefore,
 the method @code{change_capacity} exists to reallocate the sparse memory.
@@ -842,17 +842,17 @@ for (int j = 1; j < nc; j++)
               @}
             sm.data(ii) = tmp;
             sm.ridx(ii) = i;
             ii++;
           @}
       @}
     sm.cidx(j+1) = ii;
  @}
-sm.maybe_mutate ();  // If don't know a-priori the final # of nz.
+sm.maybe_mutate ();  // If don't know a priori the final # of nz.
 @end example
 
 Note that both increasing and decreasing the number of non-zero elements in
 a sparse matrix is expensive as it involves memory reallocation.  Also as
 parts of the matrix, though not its entirety, exist as old and new copies
 at the same time, additional memory is needed.  Therefore, if possible this
 should be avoided.
 
diff --git a/doc/interpreter/interp.txi b/doc/interpreter/interp.txi
--- a/doc/interpreter/interp.txi
+++ b/doc/interpreter/interp.txi
@@ -83,19 +83,19 @@ interpolation methods for a step functio
 @end ifnotinfo
 
 Fourier interpolation, is a resampling technique where a signal is
 converted to the frequency domain, padded with zeros and then
 reconverted to the time domain.
 
 @DOCSTRING(interpft)
 
-There are two significant limitations on Fourier interpolation.  Firstly,
+There are two significant limitations on Fourier interpolation.  First,
 the function signal is assumed to be periodic, and so non-periodic
-signals will be poorly represented at the edges.  Secondly, both the
+signals will be poorly represented at the edges.  Second, both the
 signal and its interpolation are required to be sampled at equispaced
 points.  An example of the use of @code{interpft} is
 
 @example
 @group
 t = 0 : 0.3 : pi; dt = t(2)-t(1);
 n = length (t); k = 100;
 ti = t(1) + [0 : k-1]*dt*n/k;
diff --git a/doc/interpreter/sparse.txi b/doc/interpreter/sparse.txi
--- a/doc/interpreter/sparse.txi
+++ b/doc/interpreter/sparse.txi
@@ -42,17 +42,17 @@ the speed of the computer and its availa
 the problem size. 
 
 There are many classes of mathematical problems which give rise to
 matrices, where a large number of the elements are zero.  In this case
 it makes sense to have a special matrix type to handle this class of
 problems where only the non-zero elements of the matrix are
 stored.  Not only does this reduce the amount of memory to store the
 matrix, but it also means that operations on this type of matrix can
-take advantage of the a-priori knowledge of the positions of the
+take advantage of the a priori knowledge of the positions of the
 non-zero elements to accelerate their calculations.
 
 A matrix type that stores only the non-zero elements is generally called
 sparse.  It is the purpose of this document to discuss the basics of the
 storage and creation of sparse matrices and the fundamental operations
 on them.
 
 @menu
@@ -68,17 +68,17 @@ on them.
 It is not strictly speaking necessary for the user to understand how
 sparse matrices are stored.  However, such an understanding will help
 to get an understanding of the size of sparse matrices.  Understanding
 the storage technique is also necessary for those users wishing to 
 create their own oct-files. 
 
 There are many different means of storing sparse matrix data.  What all
 of the methods have in common is that they attempt to reduce the complexity
-and storage given a-priori knowledge of the particular class of problems
+and storage given a priori knowledge of the particular class of problems
 that will be solved.  A good summary of the available techniques for storing
 sparse matrix is given by Saad @footnote{Y. Saad "SPARSKIT: A basic toolkit
 for sparse matrix computation", 1994,
 @url{http://www-users.cs.umn.edu/~saad/software/SPARSKIT/paper.ps}}.
 With full matrices, knowledge of the point of an element of the matrix
 within the matrix is implied by its position in the computers memory. 
 However, this is not the case for sparse matrices, and so the positions
 of the non-zero elements of the matrix must equally be stored. 
@@ -568,17 +568,17 @@ Note that the @code{sparse_auto_mutate} 
 
 @node Mathematical Considerations
 @subsubsection Mathematical Considerations
 
 The attempt has been made to make sparse matrices behave in exactly the
 same manner as there full counterparts.  However, there are certain differences
 and especially differences with other products sparse implementations.
 
-Firstly, the @qcode{"./"} and @qcode{".^"} operators must be used with care. 
+First, the @qcode{"./"} and @qcode{".^"} operators must be used with care. 
 Consider what the examples
 
 @example
 @group
   s = speye (4);
   a1 = s .^ 2;
   a2 = s .^ s;
   a3 = s .^ -2;
diff --git a/libinterp/corefcn/sparse.cc b/libinterp/corefcn/sparse.cc
--- a/libinterp/corefcn/sparse.cc
+++ b/libinterp/corefcn/sparse.cc
@@ -76,26 +76,16 @@ their values are derived from the maximu
 @var{j} as given by @code{@var{m} = max (@var{i})},\n\
 @code{@var{n} = max (@var{j})}.\n\
 \n\
 @strong{Note}: if multiple values are specified with the same\n\
 @var{i}, @var{j} indices, the corresponding values in @var{s} will\n\
 be added.  See @code{accumarray} for an example of how to produce different\n\
 behavior, such as taking the minimum instead.\n\
 \n\
-The following are all equivalent:\n\
-\n\
-@example\n\
-@group\n\
-s = sparse (i, j, s, m, n)\n\
-s = sparse (i, j, s, m, n, \"summation\")\n\
-s = sparse (i, j, s, m, n, \"sum\")\n\
-@end group\n\
-@end example\n\
-\n\
 Given the option @qcode{\"unique\"}, if more than two values are specified\n\
 for the same @var{i}, @var{j} indices, the last specified value will be\n\
 used.\n\
 \n\
 @code{sparse (@var{m}, @var{n})} is equivalent to\n\
 @code{sparse ([], [], [], @var{m}, @var{n}, 0)}\n\
 \n\
 If any of @var{sv}, @var{i} or @var{j} are scalars, they are expanded\n\
